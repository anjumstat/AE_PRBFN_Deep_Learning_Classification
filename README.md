# AE_PRBFN_Deep_Learning_Classification
### 1. Genomic Data Processing Pipeline: From Raw CDS to Curated Datasets

This repository contains a pipeline for downloading and rigorously curating Coding DNA Sequence (CDS) data from Ensembl Plants for use in genomic studies and machine learning projects.

## Pipeline Overview

### Step 1: Data Acquisition from Ensembl Plants
**Script:** Manual Download / `wget` or `rsync` commands
**Description:** CDS sequences in FASTA format were downloaded for four grass species (*Brachypodium distachyon*, *Hordeum vulgare*, *Oryza sativa*, *Triticum aestivum*) from the [Ensembl Plants](http://plants.ensembl.org/) database. This step involves:
- Navigating to the Ensembl Plants website
- Selecting the target species
- Accessing the "Gene" section
- Downloading the "CDS" FASTA files for each species
- Ensuring data version consistency (e.g., Release 60)

### Step 2: CDS Validation and Filtering
**File**  'Script\01_CDS_Validation_Pipeline.py`
**Description:** This core script performs rigorous quality control on the raw FASTA files from Ensembl through seven sequential biological validation checks:

1.  **Length Check:** Sequence length is divisible by 3.
2.  **Base Validation:** Contains only standard DNA bases (`A`, `T`, `C`, `G`).
3.  **Start Codon:** Must begin with the canonical start codon `ATG`.
4.  **Stop Codon:** Must end with a valid stop codon (`TAA`, `TAG`, or `TGA`).
5.  **Internal Stops:** Must contain no internal stop codons.
6.  **Translation:** Must successfully translate to an amino acid sequence using the standard genetic code.
7.  **Length Consistency:** The translated protein length must be consistent with the DNA sequence length.

## Usage

1.  **Acquire Data:** Download CDS FASTA files for your target species from [Ensembl Plants](http://plants.ensembl.org/).
2.  **Configure Paths:** Edit the following variables in `CDS_Validation_Pipeline.py`:
    ```python
    input_dir = r"path/to/your/raw/ensembl/fasta/files"
    output_dir = r"path/to/your/filtered/output/directory"
    ```
3.  **Run the Pipeline:**
    ```bash
    python CDS_Validation_Pipeline.py
    ```

## Outputs

*   **Filtered FASTA Files:** For each input species file, a new filtered file (suffix `_filtered.fasta`) is generated, containing only sequences that passed all biological criteria.
*   **Detailed CSV Report:** A comprehensive statistics file (`filtering_stats.csv`) logs the number of sequences discarded at each filtering step, providing full transparency into the data curation process.

## Dependencies

*   Python 3.x
*   Biopython (`pip install biopython`)

## Use Case

This end-to-end pipeline is designed for:
- Creating high-quality, reliable datasets for phylogenetic analysis
- Preparing data for molecular evolutionary studies (e.g., dN/dS calculation)
- Curating training data for machine learning models in genomics
- Generating benchmark datasets for comparative genomics

**Note:** The filtering statistics generated by this pipeline were used to create Table 1 in the associated manuscript, demonstrating the rigorous quality control applied to the training data.
### 2. Create Codon Frequency Table

**File:** `Scripts/02_Create_Codon_Frequency_Table.py`

**Purpose:** Processes the filtered FASTA files from the previous step to generate the final feature dataset for machine learning. This script calculates the absolute frequency of each codon for every individual gene sequence and combines them into a single, labeled CSV file.

**Key Operations:**
1.  **Counts Sequences:** First, it counts the number of valid sequences per species to establish a hierarchy (ordered by species with the most data).
2.  **Assigns Numerical Labels:** Species are assigned an integer label (1, 2, 3, 4) based on this count order.
3.  **Computes Codon Frequencies:** For each gene sequence, it counts the occurrence of all 64 possible codons.
4.  **Generates Feature Table:** Creates a large DataFrame where:
    - Each **row** represents a single gene sequence.
    - The **first columns** are metadata: `Species` (text name), `Label` (integer ID), `Sequence_ID`.
    - The **next 64 columns** are the absolute counts for each codon (e.g., `AAA`, `AAC`, ... `TTT`).
5.  **Saves Output:** Exports the complete dataset as a CSV file, ready to be used as input for machine learning models.

**Input:**
- Directory containing filtered FASTA files (output from `01_CDS_Validation_Pipeline.py`).

**Output:**
- A single `combined_codon_frequencies_labeled.csv` file. This is the primary feature matrix for all subsequent classification tasks.

**Usage:**
1.  Configure the `input_dir` path to point to your folder of filtered FASTA files.
2.  Run the script:
    ```bash
    python 02_Create_Codon_Frequency_Table.py
    ```
### 3. Principal Component Analysis (PCA) Visualization

**File:** `Scripts/03_PCA_RSCU.py`

**Purpose:** Generates a 2D PCA plot to visualize the separability of the four plant species based on their codon usage patterns. This script creates **Figure 1** from the associated manuscript, providing an intuitive visual assessment of whether the genomic data contains inherent, clusterable structure before model training.

**Key Operations:**
1.  **Loads Feature Data:** Reads the final CSV file generated by `02_Create_Codon_Frequency_Table.py`.
2.  **Dimensionality Reduction:** Applies Principal Component Analysis (PCA) to reduce the 64-dimensional codon frequency space down to the two most significant components (PC1 and PC2).
3.  **Visualization:** Creates a scatter plot where each point represents a gene sequence, colored by its species label. The plot includes:
    - Percentage of variance explained by each principal component.
    - A legend for species identification.
    - A clean, publication-ready style.

**Input:**
- The `4_Species_gene.csv` file (or equivalent codon frequency CSV).

**Output:**
- A PCA scatter plot displayed on-screen and/or saved to a file (e.g., `PCA_Plot.png`).

**Dependencies:**
- pandas
- scikit-learn
- matplotlib
- seaborn

**Usage:**
1.  Configure the `file_path` variable to point to your codon frequency CSV file.
2.  Run the script:
    ```bash
    python 03_PCA_RSCU.py
    ```

**Interpretation:** The resulting plot visually demonstrates the underlying structure in the data. Distinct clusters for each species provide a strong rationale for proceeding with classification models, as they indicate that codon usage is a species-specific signature.
### **04_PCA_UMAP.py**

**Purpose:** Creates a side-by-side comparative visualization of PCA and UMAP projections of the codon usage data. This script generated **Figure 3** in the associated manuscript. The figure provides a powerful visual argument for the feasibility of the classification task by showing clear species clusters in a reduced dimension.

**What it does:**
*   **Input:** Loads the codon frequency dataset (e.g., `4_Species_gene.csv`).
*   **Preprocessing:** Normalizes codon counts to relative frequencies per gene and standardizes the features.
*   **Dimensionality Reduction:** Applies two distinct techniques:
    *   **PCA (Linear):** Finds the orthogonal directions of maximum variance in the data.
    *   **UMAP (Non-linear):** A manifold learning technique that often preserves more complex, non-linear cluster structures.
*   **Output:** Produces a publication-quality, two-panel figure contrasting the two techniques. This allows for a robust assessment of the inherent clusterability of the species based on their codon usage patterns.

**Key Libraries:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`, `umap-learn`

**Usage:**
1.  Install the required UMAP library: `pip install umap-learn`
2.  Ensure the input CSV file path is correct in the script.
3.  Run the script:
    ```bash
    python Scripts/04_PCA_UMAP.py
```
### **05_Plot_Training_History.py**

**Purpose:** Visualizes the learning process of the trained Standard RBFN model by plotting accuracy and loss curves across training epochs. This script generated **Figure 4** in the associated manuscript. The figure is critical for diagnosing model behavior, confirming successful convergence, and validating that no overfitting occurred.

**What it does:**
*   **Input:** Loads the training history data (`all_histories.npy`) saved during the 10-fold cross-validation process for the Standard RBFN model.
*   **Processing:** Averages the accuracy and loss trajectories from all 10 folds to create a single, representative learning curve.
*   **Output:** Produces a publication-quality, two-panel figure:
    *   **Left Panel:** Training vs. Validation Accuracy per epoch.
    *   **Right Panel:** Training vs. Validation Loss per epoch.
*   **Interpretation:** Close convergence of the training and validation lines indicates the model learned generalizable patterns without memorizing the training data (overfitting).

**Key Libraries:** `numpy`, `matplotlib`

**Usage:**
1.  Ensure the path to the `all_histories.npy` file is correct in the script.
2.  Run the script:
    ```bash
    python Scripts/05_Plot_Training_History.py
    ```
